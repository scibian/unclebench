#!/usr/bin/env python
# -*- coding: utf-8 -*-
##############################################################################
#  This file is part of the UncleBench benchmarking tool.                    #
#        Copyright (C) 2019 EDF SA                                           #
#                                                                            #
#  UncleBench is free software: you can redistribute it and/or modify        #
#  it under the terms of the GNU General Public License as published by      #
#  the Free Software Foundation, either version 3 of the License, or         #
#  (at your option) any later version.                                       #
#                                                                            #
#  UncleBench is distributed in the hope that it will be useful,             #
#  but WITHOUT ANY WARRANTY; without even the implied warranty of            #
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the             #
#  GNU General Public License for more details.                              #
#                                                                            #
#  You should have received a copy of the GNU General Public License         #
#  along with UncleBench.  If not, see <http://www.gnu.org/licenses/>.       #
#                                                                            #
##############################################################################
# pylint: disable=invalid-name
"""
Unclebench main script
"""

import os
import sys
import argparse
import socket
import ubench.core.ubench_commands as ubench_commands
import ubench.core.ubench_config as uconfig
from ubench.release import __version__

os.environ['JUBE_EXEC_SHELL'] = '/bin/bash'

# Set all default path
uconf = uconfig.UbenchConfig()

platform_list = uconf.get_platform_list()
benchmark_list = uconf.get_benchmark_list()

default_platform = None
platform_required = True

for platform_name in platform_list:
    if platform_name in socket.gethostname().lower():
        default_platform = platform_name
        platform_required = False

# Build an argparse with a subparse for each main ubench option
parser = argparse.ArgumentParser(description='Unclebench benchmarking and reporting tool.', \
                                 formatter_class=argparse.RawTextHelpFormatter)

parser.add_argument('-v','--version', action='version', version='%(prog)s {version}'.format(version=__version__))

subparsers = parser.add_subparsers(dest='subparser_name')

## parser for fetch

fetch_help = 'Download benchmarks sources and test cases '+\
    'from benchmark defined location to /scratch/<user>/Ubench/resource '+\
    '(customizable with UBENCH_RESOURCE_DIR environment variable).'

parser_fetch = subparsers.add_parser('fetch', help=fetch_help)

parser_fetch.add_argument('-b', help='Benchmarks to fetch. Benchmark definition '+\
                          'files should be located in /usr/share/unclebench/benchmarks/'+\
                          ' (can be custumosized with UBENCH_BENCHMARK_DIR environment variable)',\
                          nargs='+', choices=benchmark_list, required=True)

# parser for run
run_help = 'Execute benchmarks in /scratch/<user>/Ubench/benchmarks directory '+\
           '(can be customized with UBENCH_RUN_DIR_BENCH directory)'

parser_run = subparsers.add_parser('run', help=run_help)

parser_run.add_argument('-p', help='Name of the test platform. '+\
                        'Platform definition files should be located '+\
                        ' in/usr/share/unclebench/platform/'+\
                        '(can be customized with UBENCH_PLATFORM_DIR environment variable)', \
                        required=platform_required, \
                        choices=platform_list, default=default_platform)

parser_run.add_argument('-b', help='Benchmarks to run. '+\
                        'Benchmark definition files should be located in'+\
                        '/usr/share/unclebench/benchmarks/ '+\
                        '(can be custumosized with UBENCH_BENCHMARK_DIR environment variable)', \
                        nargs='+', required=True, choices=benchmark_list)

parser_run.add_argument('-w', help='Nodes on which to run benchmarks '+\
                        'ex: -w 4,pocn[380,431-433]. You can also launch job on all idle nodes '+\
                        'ex: -w all4 to run a benchmark with 4 nodes jobs covering evry idle node'\
                        , nargs='*', action='append')


parser_run.add_argument('-c', '--custom-params', \
                        help='Set custom parameters. ex : --custom-params mpiv:0. '+\
                        'Use ubench listparams to know which parameters are customizable', \
                        nargs='+', default=[])

parser_run.add_argument('-f', '--file-params', \
                        help='File to set custom parameters')

'custom_params_file'
parser_run.add_argument('-e', '--execute', \
                        help='Peforms only the execution step of a benchmark', \
                        default=False, action='store_true')


# parser list
list_help = 'List existing runs information for a given benchmark.'

parser_list = subparsers.add_parser('list', help=list_help)

parser_list.add_argument('-p', help='Name of the test platform', required=platform_required, \
                                   choices=platform_list, default=default_platform)
parser_list.add_argument('-b', help='Benchmark names list', required=True, nargs='+', \
                                   choices=benchmark_list)

# parser log
parser_log = subparsers.add_parser('log', help='Print log of a benchmark run given its ID.')

parser_log.add_argument('-p', help='Name of the test platform', required=platform_required, \
                                   choices=platform_list, default=default_platform)
parser_log.add_argument('-b', help='Benchmark names list', required=True, nargs='+', \
                                   choices=benchmark_list)
parser_log.add_argument('-i', help='Benchmark run IDs', nargs='+')


# parser listparams
parser_listparams = subparsers.add_parser('listparams', \
                                          help='List customizable parameters of a benchmark.')
parser_listparams.add_argument('-p', help='Name of the test platform', required=platform_required, \
                               choices=platform_list, default=default_platform)
parser_listparams.add_argument('-b', help='Benchmark names list', required=True, nargs='+', \
                               choices=benchmark_list)
parser_listparams.add_argument('-d', help='Print default parameters values', \
                               default=False, action='store_true')

# parser result
parser_result = subparsers.add_parser('result', \
                                      help='Print raw results array from a benchmark run. '+\
                                      'To be found benchmark runs must be located in '+\
                                      '/sratch/<user>/Ubench/benchmarks directory but '+\
                                      'this path can be customized with UBENCH_RUN_DIR_BENCH '+\
                                      'environment variable')

parser_result.add_argument('-p', help='Name of the test platform', required=platform_required, \
                                   choices=platform_list, default=default_platform)
parser_result.add_argument('-b', help='Benchmark names list', required=True, nargs='+', \
                                   choices=benchmark_list)
parser_result.add_argument('-i', help='Benchmark run IDs', nargs='+')

parser_result.add_argument('-d','--debug',help='Mode to debug Unclebench', action='store_true')

# parser report
report_help = 'Build a performance report from benchmark result directories.'

parser_report = subparsers.add_parser('report', help=report_help)

parser_report.add_argument('-m','--metadata-file',help='Metadata file containing evrything needed'+\
                           'to build the report', required=True)

parser_report.add_argument('-o','--output-dir',help='Write report files in OUTPUT_DIR', \
                           required=True)


# parser compare
parser_compare = subparsers.add_parser('compare', \
                                       help='Compare results from different run directories.')

parser_compare.add_argument('-i', '--input-dirs', nargs='+', \
                            help='directories where results are to compared', required=True)

parser_compare.add_argument('-b', '--benchmark-name', \
                            help='name of the benchmark from which results are compared', required=True)

parser_compare.add_argument('-c', '--context', default=None, \
                            help='fields to use as context and should not be compared '+\
                            'but merged in the result table', required=False, nargs='+')

parser_compare.add_argument('-cc', '--compared-context', default=None, \
                            help='field to use as context and whose value should be compared '\
                            , required=False)

parser_compare.add_argument('-t', '--threshold', default=None, \
                            help='Differences under given threshold will not be printed')

# parser info
parser_info = subparsers.add_parser('info', \
                                    help='Displays path settings origin.')


# Call script according to user command
args = parser.parse_args()

# initialize class
platform_arg = ""
benchmark_arg = ""
if vars(args).has_key('p'):
    platform_arg = args.p
if vars(args).has_key('b'):
    benchmark_arg = args.b

commands = ubench_commands.UbenchCmd(platform=platform_arg, benchmark_list=benchmark_arg)

if args.subparser_name == 'run':
    options_dictionary = vars(args)
    options_dictionary['raw_cli'] = sys.argv
    commands.run(options_dictionary)
    # commands.run(w_list=args.w, customp_list=args.custom_params, execute=args.execute,raw_cli=sys.argv)
elif args.subparser_name == 'fetch':
    commands.fetch()
elif args.subparser_name == 'result':
    commands.result(args.i, args.debug)
elif args.subparser_name == 'log':
    commands.log(id_list=args.i)
elif args.subparser_name == 'list':
    commands.listb()
elif args.subparser_name == 'listparams':
    commands.list_parameters(default_values=args.d)
elif args.subparser_name == 'report':
    commands.report(args.metadata_file, args.output_dir)
elif args.subparser_name == 'compare':
    commands.compare(args.input_dirs, args.benchmark_name, \
                     (args.context,args.compared_context), args.threshold)
elif args.subparser_name == 'info':
    uconf.print_config()
